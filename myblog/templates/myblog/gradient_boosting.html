{% extends 'myblog/base.html' %}

{% block title %}Градиентный бустинг{% endblock %}

{% block content %}
  <style>
    .content-container {
      margin: 20px auto; /* Отступы со всех сторон и автоматическое выравнивание по центру */
      line-height: 2; /* Здесь 1.5 - это множитель текущего размера шрифта */
      max-width: 1000px; /* Максимальная ширина контейнера */
    }

    .img-container {
      margin-bottom: 20px; /* Дополнительный отступ снизу */
      text-align: center; /* Выравнивание по центру */
    }

    .img-container img {
      max-width: 100%; /* Максимальная ширина изображения */
      height: auto; /* Автоматическая высота */
    }
    .code-cell {
        margin-left: 0; /* Убираем отступ слева */
        margin-right: auto; /* Выравнивание по правому краю */
        width: auto; /* Ширина ячейки кода */
        border: 1px solid #ccc;
        border-radius: 2px;
        padding: 1px;
        background-color: #f7f7f7;
        font-family: monospace;
        line-height: 1.5;
        overflow-x: auto;
        text-align: left; /* Выравниваем текст по левому краю */
    }

  </style>

  <div class="content-container">
    <h2>Градиентный бустинг</h2>

    <p>
    Градиентный бустинг (Gradient Boosting) - это метод машинного обучения, который использует ансамбль слабых моделей, например, деревьев решений, для создания сильной модели прогнозирования. Он работает путем последовательного обучения новых моделей, которые исправляют ошибки предыдущих моделей. Градиентный бустинг широко применяется к задачам регрессии и классификации и обычно демонстрирует высокую точность прогнозирования.
    </p>

    <div class="img-container">
      <img class="img-fluid rounded" src="/media/boost_media/1_boost_gif.webp" alt="" width="600" height="400">
    </div>

    <p>
    Для реализации градиентного бустинга на основе датасета из Kaggle, мы используем библиотеки машинного обучения, такие как scikit-learn и XGBoost, а также фреймворк глубокого обучения TensorFlow.
    </p>

    <p>
    В качестве датасета мы выбрали данные о ценах на жилье в <a href="https://www.kaggle.com/datasets/simpleparadox/bostonhousingdataset?resource=download">Boston (Boston Housing Prices dataset)</a> из Kaggle. Разделяем данные на признаки (X) и целевую переменную (y).
    </p>

    <div class="img-container">
      <pre class="code-cell">
        <code>
        import pandas as pd
        # Загрузка данных
        data = pd. read_csv('boston-housing-dataset.csv')
        # Разделение на признаки и целевую переменную
        X = data.drop ( 'MEDV', axis=1)
        y = data ['MEDV']
        </code>
      </pre>
    </div>

    <p>
    Основной код для обучения модели градиентного бустинга с помощью библиотеки XGBoost. Модель обучается на обучающих данных (X_train_scaled, y_train) с использованием гиперпараметров, заданных при создании модели. Затем модель используется для предсказания на тестовой выборке.
    </p>

    <div class="img-container">
      <pre class="code-cell">
        <code>
        from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler import xgboost as xgb from sklearn.metrics import mean_squared_error
        # Разделение на обучающую и тестовую выборки
        X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        # Масштабирование признаков
        scaler = StandardScaler ()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler. transform (X_test)
        # Обучение модели
        mode l_xgb = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
        max_depth = 5, alpha = 10, n_estimators = 100)
        model_gb. fit(X_train_scaled, y_train)
        # Предсказание на тестовой выборке
        y_pred_gb = model_gb.predict(X_test_scaled)
        # Оценка качества модели
        mse_xgb = mean_squared_errory_test, y_pred_gb)
        print( "Mean Squared Error (XGBoost):", mse_xgb)

        Mean Squared Error (XGBoost): 7.254786596653887
        </code>
      </pre>
    </div>

    <p>
    Выводится значение среднеквадратичной ошибки (MSE), чтобы оценить качество модели. Для визуализации результатов можно построить график, где по оси X будут настоящие цены на жилье, а по оси Y - предсказанные значения целевой переменной. Чем ближе точки к диагонали, тем лучше модель.
    </p>

    <div class="img-container">
      <pre class="code-cell">
        <code>
        import matplotlib.pyplot as plt
        # Вывод графика
        plt.figure(figsize=(10, 6))
        plt. scatter(y_test, y_pred_xgb, color='blue')
        plt. title('График оценки модели (XGBoost)')
        plt.xlabel ( 'Фактические значения')
        plt.ylabel ('Предсказанные значения" )
        plt. show()
        </code>
      </pre>
    </div>

    <div class="img-container">
      <img class="img-fluid rounded" src="/media/boost_media/5_boost.png" alt="" width="600" height="400">
    </div>

    <p>
    Вывод: Градиентный бустинг с использованием библиотеки XGBoost позволяет создать модель с высокой точностью прогнозирования. Оценка качества модели показала низкое значение среднеквадратичной ошибки, что указывает на хорошее качество предсказаний. График настоящих и предсказанных значений также демонстрирует, что модель достаточно точно предсказывает целевую переменную, что подтверждает ее эффективность в данной задаче.
    </p>
  </div>
{% endblock %}
